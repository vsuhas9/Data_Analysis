{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vsuhas9/Data_Analysis/blob/dev-suhas/CyberSecurity/DDOS/NNs/Transformers/PatchTST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZIHKdrSoYCh"
      },
      "outputs": [],
      "source": [
        "#|default_exp models.PatchTST"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tsai[extras]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAol7eNKooaF",
        "outputId": "30d33140-7191-4f0c-d373-b69136515179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tsai[extras]\n",
            "  Downloading tsai-0.3.8-py3-none-any.whl (324 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.2/324.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fastai>=2.7.13 in /usr/local/lib/python3.10/dist-packages (from tsai[extras]) (2.7.13)\n",
            "Collecting pyts>=0.12.0 (from tsai[extras])\n",
            "  Downloading pyts-0.13.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting imbalanced-learn>=0.11.0 (from tsai[extras])\n",
            "  Downloading imbalanced_learn-0.11.0-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.6/235.6 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.4.8 in /usr/local/lib/python3.10/dist-packages (from tsai[extras]) (5.9.5)\n",
            "Requirement already satisfied: scikit-learn>=1.2 in /usr/local/lib/python3.10/dist-packages (from tsai[extras]) (1.2.2)\n",
            "Requirement already satisfied: torch<2.2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from tsai[extras]) (2.1.0+cu121)\n",
            "Collecting sktime>=0.10.1 (from tsai[extras])\n",
            "  Downloading sktime-0.25.0-py3-none-any.whl (21.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tsfresh>=0.18.0 (from tsai[extras])\n",
            "  Downloading tsfresh-0.20.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.3/95.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tsai[extras]) (1.5.0)\n",
            "Requirement already satisfied: nbformat>=5.1.3 in /usr/local/lib/python3.10/dist-packages (from tsai[extras]) (5.9.2)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastai>=2.7.13->tsai[extras]) (23.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastai>=2.7.13->tsai[extras]) (23.2)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from fastai>=2.7.13->tsai[extras]) (0.0.7)\n",
            "Requirement already satisfied: fastcore<1.6,>=1.5.29 in /usr/local/lib/python3.10/dist-packages (from fastai>=2.7.13->tsai[extras]) (1.5.29)\n",
            "Requirement already satisfied: torchvision>=0.11 in /usr/local/lib/python3.10/dist-packages (from fastai>=2.7.13->tsai[extras]) (0.16.0+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from fastai>=2.7.13->tsai[extras]) (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fastai>=2.7.13->tsai[extras]) (1.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fastai>=2.7.13->tsai[extras]) (2.31.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from fastai>=2.7.13->tsai[extras]) (6.0.1)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from fastai>=2.7.13->tsai[extras]) (1.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from fastai>=2.7.13->tsai[extras]) (9.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from fastai>=2.7.13->tsai[extras]) (1.11.4)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.10/dist-packages (from fastai>=2.7.13->tsai[extras]) (3.6.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn>=0.11.0->tsai[extras]) (1.23.5)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn>=0.11.0->tsai[extras]) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn>=0.11.0->tsai[extras]) (3.2.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1.3->tsai[extras]) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1.3->tsai[extras]) (4.19.2)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1.3->tsai[extras]) (5.7.1)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1.3->tsai[extras]) (5.7.1)\n",
            "Requirement already satisfied: numba>=0.55.2 in /usr/local/lib/python3.10/dist-packages (from pyts>=0.12.0->tsai[extras]) (0.58.1)\n",
            "Collecting scikit-base<0.7.0 (from sktime>=0.10.1->tsai[extras])\n",
            "  Downloading scikit_base-0.6.2-py3-none-any.whl (122 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/122.9 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.10->tsai[extras]) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.10->tsai[extras]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.10->tsai[extras]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.10->tsai[extras]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.10->tsai[extras]) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.10->tsai[extras]) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.10->tsai[extras]) (2.1.0)\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.10/dist-packages (from tsfresh>=0.18.0->tsai[extras]) (0.14.1)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tsfresh>=0.18.0->tsai[extras]) (0.5.6)\n",
            "Requirement already satisfied: tqdm>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from tsfresh>=0.18.0->tsai[extras]) (4.66.1)\n",
            "Requirement already satisfied: dask[dataframe]>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tsfresh>=0.18.0->tsai[extras]) (2023.8.1)\n",
            "Requirement already satisfied: distributed>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tsfresh>=0.18.0->tsai[extras]) (2023.8.1)\n",
            "Collecting stumpy>=1.7.2 (from tsfresh>=0.18.0->tsai[extras])\n",
            "  Downloading stumpy-1.12.0-py3-none-any.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.1/169.1 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from tsfresh>=0.18.0->tsai[extras]) (2.2.1)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.9.0->tsfresh>=0.18.0->tsai[extras]) (8.1.7)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.9.0->tsfresh>=0.18.0->tsai[extras]) (1.4.1)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.9.0->tsfresh>=0.18.0->tsai[extras]) (0.12.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.9.0->tsfresh>=0.18.0->tsai[extras]) (7.0.1)\n",
            "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.18.0->tsai[extras]) (1.0.0)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.18.0->tsai[extras]) (1.0.7)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.18.0->tsai[extras]) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.18.0->tsai[extras]) (3.0.0)\n",
            "Requirement already satisfied: tornado>=6.0.4 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.18.0->tsai[extras]) (6.3.2)\n",
            "Requirement already satisfied: urllib3>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.18.0->tsai[extras]) (2.0.7)\n",
            "Requirement already satisfied: zict>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.18.0->tsai[extras]) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.2,>=1.10->tsai[extras]) (2.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1.3->tsai[extras]) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1.3->tsai[extras]) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1.3->tsai[extras]) (0.32.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1.3->tsai[extras]) (0.17.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.55.2->pyts>=0.12.0->tsai[extras]) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fastai>=2.7.13->tsai[extras]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fastai>=2.7.13->tsai[extras]) (2023.3.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.4.1->tsfresh>=0.18.0->tsai[extras]) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fastai>=2.7.13->tsai[extras]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fastai>=2.7.13->tsai[extras]) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fastai>=2.7.13->tsai[extras]) (2023.11.17)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai>=2.7.13->tsai[extras]) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai>=2.7.13->tsai[extras]) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai>=2.7.13->tsai[extras]) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai>=2.7.13->tsai[extras]) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai>=2.7.13->tsai[extras]) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai>=2.7.13->tsai[extras]) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai>=2.7.13->tsai[extras]) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai>=2.7.13->tsai[extras]) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai>=2.7.13->tsai[extras]) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai>=2.7.13->tsai[extras]) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai>=2.7.13->tsai[extras]) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai>=2.7.13->tsai[extras]) (6.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai>=2.7.13->tsai[extras]) (1.10.13)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai>=2.7.13->tsai[extras]) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai>=2.7.13->tsai[extras]) (3.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->nbformat>=5.1.3->tsai[extras]) (4.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai>=2.7.13->tsai[extras]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai>=2.7.13->tsai[extras]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai>=2.7.13->tsai[extras]) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai>=2.7.13->tsai[extras]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai>=2.7.13->tsai[extras]) (3.1.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.2,>=1.10->tsai[extras]) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask[dataframe]>=2.9.0->tsfresh>=0.18.0->tsai[extras]) (3.17.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy<4->fastai>=2.7.13->tsai[extras]) (0.1.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4->fastai>=2.7.13->tsai[extras]) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4->fastai>=2.7.13->tsai[extras]) (0.1.4)\n",
            "Installing collected packages: scikit-base, stumpy, sktime, pyts, imbalanced-learn, tsfresh, tsai\n",
            "  Attempting uninstall: imbalanced-learn\n",
            "    Found existing installation: imbalanced-learn 0.10.1\n",
            "    Uninstalling imbalanced-learn-0.10.1:\n",
            "      Successfully uninstalled imbalanced-learn-0.10.1\n",
            "Successfully installed imbalanced-learn-0.11.0 pyts-0.13.0 scikit-base-0.6.2 sktime-0.25.0 stumpy-1.12.0 tsai-0.3.8 tsfresh-0.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdMaGMWEoYCi"
      },
      "source": [
        "# PatchTST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mce0AIlvoYCj"
      },
      "source": [
        "This is an unofficial PyTorch implementation of PatchTST created by Ignacio Oguiza (oguiza@timeseriesAI.co) based on:\n",
        "\n",
        "In this notebook, we are going to use a new state-of-the-art model called PatchTST (Nie et al, 2022) to create a long-term time series forecast.\n",
        "\n",
        "\n",
        "\n",
        "Here are some paper details:\n",
        "\n",
        "* Nie, Y., Nguyen, N. H., Sinthong, P., & Kalagnanam, J. (2022). **A Time Series is Worth 64 Words: Long-term Forecasting with Transformers.** arXiv preprint arXiv:2211.14730.\n",
        "* Official implementation:: https://github.com/yuqinie98/PatchTST\n",
        "\n",
        "```bash\n",
        "@article{Yuqietal-2022-PatchTST,\n",
        "  title={A Time Series is Worth 64 Words: Long-term Forecasting with Transformers},\n",
        "  author={Yuqi Nie and\n",
        "          Nam H. Nguyen and\n",
        "          Phanwadee Sinthong and\n",
        "          Jayant Kalagnanam},\n",
        "  journal={arXiv preprint arXiv:2211.14730},\n",
        "  year={2022}\n",
        "}\n",
        "```\n",
        "\n",
        "PatchTST has shown some impressive results across some of the most widely used long-term datasets for benchmarking:\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5I4oeiNsoYCk"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "import warnings\n",
        "from typing import Optional\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from tsai.models.layers import Transpose, get_act_fn, RevIN\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "en-DY8HpoYCl"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "class MovingAverage(nn.Module):\n",
        "    \"Moving average block to highlight the trend of time series\"\n",
        "\n",
        "    def __init__(self,\n",
        "         kernel_size:int,  # the size of the window\n",
        "         ):\n",
        "        super().__init__()\n",
        "        padding_left = (kernel_size - 1) // 2\n",
        "        padding_right = kernel_size - padding_left - 1\n",
        "        self.padding = torch.nn.ReplicationPad1d((padding_left, padding_right))\n",
        "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=1)\n",
        "\n",
        "    def forward(self, x:Tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: torch.Tensor shape: [bs x seq_len x features]\n",
        "        \"\"\"\n",
        "        return self.avg(self.padding(x))\n",
        "\n",
        "\n",
        "class SeriesDecomposition(nn.Module):\n",
        "    \"Series decomposition block\"\n",
        "\n",
        "    def __init__(self,\n",
        "         kernel_size:int,  # the size of the window\n",
        "         ):\n",
        "        super().__init__()\n",
        "        self.moving_avg = MovingAverage(kernel_size)\n",
        "\n",
        "    def forward(self, x:Tensor):\n",
        "        \"\"\" Args:\n",
        "            x: torch.Tensor shape: [bs x seq_len x features]\n",
        "        \"\"\"\n",
        "        moving_mean = self.moving_avg(x)\n",
        "        residual = x - moving_mean\n",
        "        return residual, moving_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpvqQSMQoYCo"
      },
      "outputs": [],
      "source": [
        "#|exporti\n",
        "class _ScaledDotProductAttention(nn.Module):\n",
        "    r\"\"\"Scaled Dot-Product Attention module (Attention is all you need by Vaswani et al., 2017) with\n",
        "    optional residual attention from previous layer\n",
        "    Realformer: Transformer likes residual attention by He et al, 2020\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_heads, attn_dropout=0., res_attention=False):\n",
        "        super().__init__()\n",
        "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
        "        self.res_attention = res_attention\n",
        "        head_dim = d_model // n_heads\n",
        "        self.scale = nn.Parameter(torch.tensor(\n",
        "            head_dim ** -0.5), requires_grad=False)\n",
        "\n",
        "    def forward(self, q:Tensor, k:Tensor, v:Tensor, prev:Optional[Tensor]=None):\n",
        "        '''\n",
        "        Input shape:\n",
        "            q               : [bs x n_heads x max_q_len x d_k] # d_k = d_model // n_heads\n",
        "            k               : [bs x n_heads x d_k x seq_len]   # d_k = d_model // n_heads\n",
        "            v               : [bs x n_heads x seq_len x d_v]   # d_v = d_model // n_heads\n",
        "            prev            : [bs x n_heads x q_len x seq_len]\n",
        "        Output shape:\n",
        "            output          :  [bs x n_heads x q_len x d_v]    # d_v = d_model // n_heads\n",
        "            attn            : [bs x n_heads x q_len x seq_len]\n",
        "            scores          : [bs x n_heads x q_len x seq_len]\n",
        "        '''\n",
        "\n",
        "        # Scaled MatMul (q, k) - similarity scores for all pairs of positions in an input sequence\n",
        "        # attn_scores : [bs x n_heads x max_q_len x q_len]\n",
        "        attn_scores = torch.matmul(q, k) * self.scale\n",
        "\n",
        "        # Add pre-softmax attention scores from the previous layer (optional)\n",
        "        if prev is not None:\n",
        "            attn_scores = attn_scores + prev\n",
        "\n",
        "        # normalize the attention weights\n",
        "        # attn_weights   : [bs x n_heads x max_q_len x q_len]\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "        attn_weights = self.attn_dropout(attn_weights)\n",
        "\n",
        "        # compute the new values given the attention weights\n",
        "        # output: [bs x n_heads x max_q_len x d_v]\n",
        "        output = torch.matmul(attn_weights, v)\n",
        "\n",
        "        if self.res_attention:\n",
        "            return output, attn_weights, attn_scores\n",
        "        else:\n",
        "            return output, attn_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VltugkrGoYCp"
      },
      "outputs": [],
      "source": [
        "#|exporti\n",
        "class _MultiheadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_k=None, d_v=None, res_attention=False, attn_dropout=0., proj_dropout=0., qkv_bias=True):\n",
        "        \"Multi Head Attention Layer\"\n",
        "\n",
        "        super().__init__()\n",
        "        d_k = d_v = d_model // n_heads\n",
        "\n",
        "        self.n_heads, self.d_k, self.d_v = n_heads, d_k, d_v\n",
        "\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=qkv_bias)\n",
        "\n",
        "        # Scaled Dot-Product Attention (multiple heads)\n",
        "        self.res_attention = res_attention\n",
        "        self.sdp_attn = _ScaledDotProductAttention(\n",
        "            d_model, n_heads, attn_dropout=attn_dropout, res_attention=self.res_attention)\n",
        "\n",
        "        # Poject output\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(n_heads * d_v, d_model), nn.Dropout(proj_dropout))\n",
        "\n",
        "    def forward(self, Q:Tensor, K:Optional[Tensor]=None, V:Optional[Tensor]=None, prev:Optional[Tensor]=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            Q:       [batch_size (bs) x max_q_len x d_model]\n",
        "            K, V:    [batch_size (bs) x q_len x d_model]\n",
        "            mask:    [q_len x q_len]\n",
        "        \"\"\"\n",
        "\n",
        "        bs = Q.size(0)\n",
        "        if K is None:\n",
        "            K = Q\n",
        "        if V is None:\n",
        "            V = Q\n",
        "\n",
        "        # Linear (+ split in multiple heads)\n",
        "        q_s = self.W_Q(Q).view(bs, -1, self.n_heads, self.d_k).transpose(1, 2) # q_s: [bs x n_heads x max_q_len x d_k]\n",
        "        k_s = self.W_K(K).view(bs, -1, self.n_heads, self.d_k).permute(0, 2, 3, 1) # k_s: [bs x n_heads x d_k x q_len] - transpose(1,2) + transpose(2,3)\n",
        "        v_s = self.W_V(V).view(bs, -1, self.n_heads, self.d_v).transpose(1, 2) # v_s: [bs x n_heads x q_len x d_v]\n",
        "\n",
        "        # Apply Scaled Dot-Product Attention (multiple heads)\n",
        "        if self.res_attention:\n",
        "            output, attn_weights, attn_scores = self.sdp_attn(q_s, k_s, v_s, prev=prev)\n",
        "        else:\n",
        "            output, attn_weights = self.sdp_attn(q_s, k_s, v_s)\n",
        "        # output: [bs x n_heads x q_len x d_v], attn: [bs x n_heads x q_len x q_len], scores: [bs x n_heads x max_q_len x q_len]\n",
        "\n",
        "        # back to the original inputs dimensions\n",
        "        output = output.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * self.d_v)  # output: [bs x q_len x n_heads * d_v]\n",
        "        output = self.to_out(output)\n",
        "\n",
        "        if self.res_attention:\n",
        "            return output, attn_weights, attn_scores\n",
        "        else:\n",
        "            return output, attn_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfJ2p1UKoYCq"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "class Flatten_Head(nn.Module):\n",
        "    def __init__(self, individual, n_vars, nf, pred_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        if isinstance(pred_dim, (tuple, list)):\n",
        "            pred_dim = pred_dim[-1]\n",
        "        self.individual = individual\n",
        "        self.n = n_vars if individual else 1\n",
        "        self.nf, self.pred_dim = nf, pred_dim\n",
        "\n",
        "        if individual:\n",
        "            self.layers = nn.ModuleList()\n",
        "            for i in range(self.n):\n",
        "                self.layers.append(nn.Sequential(nn.Flatten(start_dim=-2), nn.Linear(nf, pred_dim)))\n",
        "        else:\n",
        "            self.layer = nn.Sequential(nn.Flatten(start_dim=-2), nn.Linear(nf, pred_dim))\n",
        "\n",
        "    def forward(self, x:Tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [bs x nvars x d_model x n_patch]\n",
        "            output: [bs x nvars x pred_dim]\n",
        "        \"\"\"\n",
        "        if self.individual:\n",
        "            x_out = []\n",
        "            for i, layer in enumerate(self.layers):\n",
        "                x_out.append(layer(x[:, i]))\n",
        "            x = torch.stack(x_out, dim=1)\n",
        "            return x\n",
        "        else:\n",
        "            return self.layer(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jxxUeY-oYCr"
      },
      "outputs": [],
      "source": [
        "#|exporti\n",
        "class _TSTiEncoderLayer(nn.Module):\n",
        "    def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=256, store_attn=False,\n",
        "                 norm='BatchNorm', attn_dropout=0, dropout=0., bias=True, activation=\"gelu\", res_attention=False, pre_norm=False):\n",
        "        super().__init__()\n",
        "        assert not d_model%n_heads, f\"d_model ({d_model}) must be divisible by n_heads ({n_heads})\"\n",
        "        d_k = d_model // n_heads if d_k is None else d_k\n",
        "        d_v = d_model // n_heads if d_v is None else d_v\n",
        "\n",
        "        # Multi-Head attention\n",
        "        self.res_attention = res_attention\n",
        "        self.self_attn = _MultiheadAttention(d_model, n_heads, d_k, d_v, attn_dropout=attn_dropout,\n",
        "                                             proj_dropout=dropout, res_attention=res_attention)\n",
        "\n",
        "        # Add & Norm\n",
        "        self.dropout_attn = nn.Dropout(dropout)\n",
        "        if \"batch\" in norm.lower():\n",
        "            self.norm_attn = nn.Sequential(Transpose(1,2), nn.BatchNorm1d(d_model), Transpose(1,2))\n",
        "        else:\n",
        "            self.norm_attn = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Position-wise Feed-Forward\n",
        "        self.ff = nn.Sequential(nn.Linear(d_model, d_ff, bias=bias),\n",
        "                                get_act_fn(activation),\n",
        "                                nn.Dropout(dropout),\n",
        "                                nn.Linear(d_ff, d_model, bias=bias))\n",
        "\n",
        "        # Add & Norm\n",
        "        self.dropout_ffn = nn.Dropout(dropout)\n",
        "        if \"batch\" in norm.lower():\n",
        "            self.norm_ffn = nn.Sequential(Transpose(1,2), nn.BatchNorm1d(d_model), Transpose(1,2))\n",
        "        else:\n",
        "            self.norm_ffn = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.pre_norm = pre_norm\n",
        "        self.store_attn = store_attn\n",
        "\n",
        "\n",
        "    def forward(self, src:Tensor, prev:Optional[Tensor]=None):\n",
        "\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: [bs x q_len x d_model]\n",
        "        \"\"\"\n",
        "\n",
        "        # Multi-Head attention sublayer\n",
        "        if self.pre_norm:\n",
        "            src = self.norm_attn(src)\n",
        "        ## Multi-Head attention\n",
        "        if self.res_attention:\n",
        "            src2, attn, scores = self.self_attn(src, src, src, prev)\n",
        "        else:\n",
        "            src2, attn = self.self_attn(src, src, src)\n",
        "        if self.store_attn:\n",
        "            self.attn = attn\n",
        "        ## Add & Norm\n",
        "        src = src + self.dropout_attn(src2) # Add: residual connection with residual dropout\n",
        "        if not self.pre_norm:\n",
        "            src = self.norm_attn(src)\n",
        "\n",
        "        # Feed-forward sublayer\n",
        "        if self.pre_norm:\n",
        "            src = self.norm_ffn(src)\n",
        "        ## Position-wise Feed-Forward\n",
        "        src2 = self.ff(src)\n",
        "        ## Add & Norm\n",
        "        src = src + self.dropout_ffn(src2) # Add: residual connection with residual dropout\n",
        "        if not self.pre_norm:\n",
        "            src = self.norm_ffn(src)\n",
        "\n",
        "        if self.res_attention:\n",
        "            return src, scores\n",
        "        else:\n",
        "            return src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcRu_zDSoYCt"
      },
      "outputs": [],
      "source": [
        "#|exporti\n",
        "class _TSTiEncoder(nn.Module):  #i means channel-independent\n",
        "    def __init__(self, c_in, patch_num, patch_len, n_layers=3, d_model=128, n_heads=16, d_k=None, d_v=None,\n",
        "                 d_ff=256, norm='BatchNorm', attn_dropout=0., dropout=0., act=\"gelu\", store_attn=False,\n",
        "                 res_attention=True, pre_norm=False):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_num = patch_num\n",
        "        self.patch_len = patch_len\n",
        "\n",
        "        # Input encoding\n",
        "        q_len = patch_num\n",
        "        self.W_P = nn.Linear(patch_len, d_model) # Eq 1: projection of feature vectors onto a d-dim vector space\n",
        "        self.seq_len = q_len\n",
        "\n",
        "        # Positional encoding\n",
        "        W_pos = torch.empty((q_len, d_model))\n",
        "        nn.init.uniform_(W_pos, -0.02, 0.02)\n",
        "        self.W_pos = nn.Parameter(W_pos)\n",
        "\n",
        "        # Residual dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Encoder\n",
        "        self.layers = nn.ModuleList([_TSTiEncoderLayer(q_len, d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm,\n",
        "                                                      attn_dropout=attn_dropout, dropout=dropout,\n",
        "                                                      activation=act, res_attention=res_attention,\n",
        "                                                      pre_norm=pre_norm, store_attn=store_attn) for i in range(n_layers)])\n",
        "        self.res_attention = res_attention\n",
        "\n",
        "\n",
        "    def forward(self, x:Tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [bs x nvars x patch_len x patch_num]\n",
        "        \"\"\"\n",
        "\n",
        "        n_vars = x.shape[1]\n",
        "        # Input encoding\n",
        "        x = x.permute(0,1,3,2)                                                   # x: [bs x nvars x patch_num x patch_len]\n",
        "        x = self.W_P(x)                                                          # x: [bs x nvars x patch_num x d_model]\n",
        "\n",
        "        x = torch.reshape(x, (x.shape[0]*x.shape[1],x.shape[2],x.shape[3]))      # x: [bs * nvars x patch_num x d_model]\n",
        "        x = self.dropout(x + self.W_pos)                                         # x: [bs * nvars x patch_num x d_model]\n",
        "\n",
        "        # Encoder\n",
        "        if self.res_attention:\n",
        "            scores = None\n",
        "            for mod in self.layers:\n",
        "                x, scores = mod(x, prev=scores)\n",
        "        else:\n",
        "            for mod in self.layers: x = mod(x)\n",
        "        x = torch.reshape(x, (-1,n_vars,x.shape[-2],x.shape[-1]))                # x: [bs x nvars x patch_num x d_model]\n",
        "        x = x.permute(0,1,3,2)                                                   # x: [bs x nvars x d_model x patch_num]\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FL3uLOMYoYCu"
      },
      "outputs": [],
      "source": [
        "#|exporti\n",
        "class _PatchTST_backbone(nn.Module):\n",
        "    def __init__(self, c_in, seq_len, pred_dim, patch_len, stride,\n",
        "                 n_layers=3, d_model=128, n_heads=16, d_k=None, d_v=None,\n",
        "                 d_ff=256, norm='BatchNorm', attn_dropout=0., dropout=0.,\n",
        "                 act=\"gelu\", res_attention=True, pre_norm=False, store_attn=False,\n",
        "                 padding_patch=True, individual=False,\n",
        "                 revin=True, affine=True, subtract_last=False):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # RevIn\n",
        "        self.revin = revin\n",
        "        self.revin_layer = RevIN(c_in, affine=affine, subtract_last=subtract_last)\n",
        "\n",
        "        # # Patching\n",
        "        self.patch_len = patch_len\n",
        "        self.stride = stride\n",
        "        self.padding_patch = padding_patch\n",
        "        patch_num = int((seq_len - patch_len) / stride + 1) + 1\n",
        "        self.patch_num = patch_num\n",
        "        self.padding_patch_layer = nn.ReplicationPad1d((stride, 0)) # original padding at the end\n",
        "\n",
        "        # Unfold\n",
        "        self.unfold = nn.Unfold(kernel_size=(1, patch_len), stride=stride)\n",
        "        self.patch_len = patch_len\n",
        "\n",
        "        # Backbone\n",
        "        self.backbone = _TSTiEncoder(c_in, patch_num=patch_num, patch_len=patch_len,\n",
        "                                     n_layers=n_layers, d_model=d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff,\n",
        "                                     attn_dropout=attn_dropout, dropout=dropout, act=act,\n",
        "                                     res_attention=res_attention, pre_norm=pre_norm, store_attn=store_attn)\n",
        "\n",
        "        # Head\n",
        "        self.head_nf = d_model * patch_num\n",
        "        self.n_vars = c_in\n",
        "        self.individual = individual\n",
        "        self.head = Flatten_Head(self.individual, self.n_vars, self.head_nf, pred_dim)\n",
        "\n",
        "\n",
        "    def forward(self, z:Tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            z: [bs x c_in x seq_len]\n",
        "        \"\"\"\n",
        "\n",
        "        # norm\n",
        "        if self.revin:\n",
        "            z = self.revin_layer(z, torch.tensor(True, dtype=torch.bool))\n",
        "\n",
        "        # do patching\n",
        "        z = self.padding_patch_layer(z)\n",
        "        b, c, s = z.size()\n",
        "        z = z.reshape(-1, 1, 1, s)\n",
        "        z = self.unfold(z)\n",
        "        z = z.permute(0, 2, 1).reshape(b, c, -1, self.patch_len).permute(0, 1, 3, 2)\n",
        "\n",
        "        # model\n",
        "        z = self.backbone(z) # z: [bs x nvars x d_model x patch_num]\n",
        "        z = self.head(z) # z: [bs x nvars x pred_dim]\n",
        "\n",
        "        # denorm\n",
        "        if self.revin:\n",
        "            z = self.revin_layer(z, torch.tensor(False, dtype=torch.bool))\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhUuOYQWoYCv"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "class PatchTST(nn.Module):\n",
        "    def __init__(self,\n",
        "         c_in,  # number of input channels\n",
        "         c_out, # used for compatibility\n",
        "         seq_len,  # input sequence length\n",
        "         pred_dim=None,  # prediction sequence length\n",
        "         n_layers=2,  # number of encoder layers\n",
        "         n_heads=8,  # number of heads\n",
        "         d_model=512,  # dimension of model\n",
        "         d_ff=2048,  # dimension of fully connected network (fcn)\n",
        "         dropout=0.05,  # dropout applied to all linear layers in the encoder\n",
        "         attn_dropout=0.,  # dropout applied to the attention scores\n",
        "         patch_len=16,  # patch_len\n",
        "         stride=8,  # stride\n",
        "         padding_patch=True,  # flag to indicate if padded is added if necessary\n",
        "         revin=True,  # RevIN\n",
        "         affine=False,  # RevIN affine\n",
        "         individual=False,  # individual head\n",
        "         subtract_last=False,  # subtract_last\n",
        "         decomposition=False,  # apply decomposition\n",
        "         kernel_size=25,  # decomposition kernel size\n",
        "         activation=\"gelu\", # activation function of intermediate layer, relu or gelu.\n",
        "         norm='BatchNorm',  # type of normalization layer used in the encoder\n",
        "         pre_norm=False, # flag to indicate if normalization is applied as the first step in the sublayers\n",
        "         res_attention=True,  # flag to indicate if Residual MultiheadAttention should be used\n",
        "         store_attn=False,  # can be used to visualize attention weights\n",
        "         ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # model\n",
        "        if pred_dim is None:\n",
        "            pred_dim = seq_len\n",
        "\n",
        "        self.decomposition = decomposition\n",
        "        if self.decomposition:\n",
        "            self.decomp_module = SeriesDecomposition(kernel_size)\n",
        "            self.model_trend = _PatchTST_backbone(c_in=c_in, seq_len=seq_len, pred_dim=pred_dim,\n",
        "                                                  patch_len=patch_len, stride=stride, n_layers=n_layers, d_model=d_model,\n",
        "                                                  n_heads=n_heads, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout,\n",
        "                                                  dropout=dropout, act=activation, res_attention=res_attention, pre_norm=pre_norm,\n",
        "                                                  store_attn=store_attn, padding_patch=padding_patch,\n",
        "                                                  individual=individual, revin=revin, affine=affine, subtract_last=subtract_last)\n",
        "            self.model_res = _PatchTST_backbone(c_in=c_in, seq_len=seq_len, pred_dim=pred_dim,\n",
        "                                                patch_len=patch_len, stride=stride, n_layers=n_layers, d_model=d_model,\n",
        "                                                n_heads=n_heads, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout,\n",
        "                                                dropout=dropout, act=activation, res_attention=res_attention, pre_norm=pre_norm,\n",
        "                                                store_attn=store_attn, padding_patch=padding_patch,\n",
        "                                                individual=individual, revin=revin, affine=affine, subtract_last=subtract_last)\n",
        "            self.patch_num = self.model_trend.patch_num\n",
        "        else:\n",
        "            self.model = _PatchTST_backbone(c_in=c_in, seq_len=seq_len, pred_dim=pred_dim,\n",
        "                                            patch_len=patch_len, stride=stride, n_layers=n_layers, d_model=d_model,\n",
        "                                            n_heads=n_heads, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout,\n",
        "                                            dropout=dropout, act=activation, res_attention=res_attention, pre_norm=pre_norm,\n",
        "                                            store_attn=store_attn, padding_patch=padding_patch,\n",
        "                                            individual=individual, revin=revin, affine=affine, subtract_last=subtract_last)\n",
        "            self.patch_num = self.model.patch_num\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Args:\n",
        "            x: rank 3 tensor with shape [batch size x features x sequence length]\n",
        "        \"\"\"\n",
        "        if self.decomposition:\n",
        "            res_init, trend_init = self.decomp_module(x)\n",
        "            res = self.model_res(res_init)\n",
        "            trend = self.model_trend(trend_init)\n",
        "            x = res + trend\n",
        "        else:\n",
        "            x = self.model(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eceKGGbaoYCw"
      },
      "outputs": [],
      "source": [
        "from fastcore.test import test_eq\n",
        "from tsai.models.utils import count_parameters\n",
        "\n",
        "bs = 32\n",
        "c_in = 1  # aka channels, features, variables, dimensions\n",
        "c_out = 1\n",
        "seq_len = 128\n",
        "pred_dim = 1\n",
        "\n",
        "\n",
        "arch_config=dict(\n",
        "        n_layers=1,  # number of encoder layers\n",
        "        n_heads=8,  # number of heads\n",
        "        d_model=128,  # dimension of model\n",
        "        d_ff=256,  # dimension of fully connected network (fcn)\n",
        "        attn_dropout=0.2,\n",
        "        dropout=0.2,  # dropout applied to all linear layers in the encoder\n",
        "        patch_len=32,  # patch_len\n",
        "        stride=16,  # stride\n",
        "    )\n",
        "\n",
        "model = PatchTST(c_in, c_out, seq_len, pred_dim, **arch_config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Function to create sequences and labels\n",
        "def create_sequences(data, seq_length):\n",
        "    xs = []\n",
        "    ys = []\n",
        "\n",
        "    for i in range(len(data) - seq_length):\n",
        "        x = data['c4'].iloc[i:(i + seq_length)].values\n",
        "        y = data[' label'].iloc[i + seq_length]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "# Load the training CSV file\n",
        "train_file_path = '/content/ddos_data_train.csv'  # Replace with your file path\n",
        "train_data = pd.read_csv(train_file_path)\n",
        "\n",
        "# Normalizing the 'c4' column in training data\n",
        "scaler = MinMaxScaler()\n",
        "train_data['c4'] = scaler.fit_transform(train_data[['c4']])\n",
        "\n",
        "# Creating sequences of length 60 for training data\n",
        "X_train, y_train = create_sequences(train_data, seq_len)\n",
        "\n",
        "# Converting to PyTorch tensors and reshaping for training data\n",
        "X_train_tensor = torch.tensor(X_train).float().unsqueeze(1)\n",
        "y_train_tensor = torch.tensor(y_train).float()\n",
        "\n",
        "# Creating a DataLoader for the training set\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=False, drop_last=True)"
      ],
      "metadata": {
        "id": "wPIPsHIyojjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "\n",
        "def fit(model, train_dataloader, epochs=10, learning_rate=1e-3):\n",
        "    # Loss Function\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for xb, yb in train_dataloader:\n",
        "            # Forward pass\n",
        "            outputs = model(xb)\n",
        "            loss = criterion(outputs.squeeze() , yb)  # Adjust shape of yb if necessary\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_dataloader)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Training the Model\n",
        "fit(model, train_dataloader, epochs=100, learning_rate=1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_47wsBhErJl1",
        "outputId": "ebd0de57-3e79-4900-a0af-ec66b35292dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 0.8258\n",
            "Epoch [2/100], Loss: 0.8193\n",
            "Epoch [3/100], Loss: 0.8017\n",
            "Epoch [4/100], Loss: 0.7890\n",
            "Epoch [5/100], Loss: 0.7801\n",
            "Epoch [6/100], Loss: 0.7650\n",
            "Epoch [7/100], Loss: 0.7586\n",
            "Epoch [8/100], Loss: 0.7540\n",
            "Epoch [9/100], Loss: 0.7336\n",
            "Epoch [10/100], Loss: 0.7266\n",
            "Epoch [11/100], Loss: 0.7106\n",
            "Epoch [12/100], Loss: 0.6934\n",
            "Epoch [13/100], Loss: 0.6759\n",
            "Epoch [14/100], Loss: 0.6504\n",
            "Epoch [15/100], Loss: 0.6205\n",
            "Epoch [16/100], Loss: 0.5851\n",
            "Epoch [17/100], Loss: 0.5625\n",
            "Epoch [18/100], Loss: 0.5243\n",
            "Epoch [19/100], Loss: 0.4797\n",
            "Epoch [20/100], Loss: 0.4577\n",
            "Epoch [21/100], Loss: 0.4656\n",
            "Epoch [22/100], Loss: 0.4305\n",
            "Epoch [23/100], Loss: 0.3931\n",
            "Epoch [24/100], Loss: 0.4007\n",
            "Epoch [25/100], Loss: 0.3769\n",
            "Epoch [26/100], Loss: 0.3665\n",
            "Epoch [27/100], Loss: 0.3776\n",
            "Epoch [28/100], Loss: 0.3630\n",
            "Epoch [29/100], Loss: 0.3829\n",
            "Epoch [30/100], Loss: 0.3503\n",
            "Epoch [31/100], Loss: 0.3447\n",
            "Epoch [32/100], Loss: 0.3571\n",
            "Epoch [33/100], Loss: 0.3443\n",
            "Epoch [34/100], Loss: 0.3196\n",
            "Epoch [35/100], Loss: 0.3142\n",
            "Epoch [36/100], Loss: 0.2991\n",
            "Epoch [37/100], Loss: 0.3383\n",
            "Epoch [38/100], Loss: 0.3243\n",
            "Epoch [39/100], Loss: 0.3025\n",
            "Epoch [40/100], Loss: 0.3072\n",
            "Epoch [41/100], Loss: 0.2962\n",
            "Epoch [42/100], Loss: 0.3269\n",
            "Epoch [43/100], Loss: 0.3213\n",
            "Epoch [44/100], Loss: 0.3455\n",
            "Epoch [45/100], Loss: 0.3129\n",
            "Epoch [46/100], Loss: 0.3072\n",
            "Epoch [47/100], Loss: 0.3240\n",
            "Epoch [48/100], Loss: 0.3106\n",
            "Epoch [49/100], Loss: 0.3823\n",
            "Epoch [50/100], Loss: 0.3728\n",
            "Epoch [51/100], Loss: 0.3790\n",
            "Epoch [52/100], Loss: 0.3289\n",
            "Epoch [53/100], Loss: 0.3260\n",
            "Epoch [54/100], Loss: 0.3204\n",
            "Epoch [55/100], Loss: 0.3369\n",
            "Epoch [56/100], Loss: 0.3338\n",
            "Epoch [57/100], Loss: 0.3119\n",
            "Epoch [58/100], Loss: 0.3594\n",
            "Epoch [59/100], Loss: 0.3176\n",
            "Epoch [60/100], Loss: 0.3592\n",
            "Epoch [61/100], Loss: 0.3710\n",
            "Epoch [62/100], Loss: 0.3891\n",
            "Epoch [63/100], Loss: 0.3570\n",
            "Epoch [64/100], Loss: 0.3426\n",
            "Epoch [65/100], Loss: 0.3236\n",
            "Epoch [66/100], Loss: 0.3030\n",
            "Epoch [67/100], Loss: 0.3339\n",
            "Epoch [68/100], Loss: 0.3492\n",
            "Epoch [69/100], Loss: 0.3119\n",
            "Epoch [70/100], Loss: 0.3105\n",
            "Epoch [71/100], Loss: 0.3631\n",
            "Epoch [72/100], Loss: 0.3549\n",
            "Epoch [73/100], Loss: 0.3769\n",
            "Epoch [74/100], Loss: 0.3696\n",
            "Epoch [75/100], Loss: 0.3607\n",
            "Epoch [76/100], Loss: 0.3308\n",
            "Epoch [77/100], Loss: 0.2901\n",
            "Epoch [78/100], Loss: 0.3217\n",
            "Epoch [79/100], Loss: 0.3471\n",
            "Epoch [80/100], Loss: 0.3202\n",
            "Epoch [81/100], Loss: 0.3465\n",
            "Epoch [82/100], Loss: 0.3607\n",
            "Epoch [83/100], Loss: 0.3124\n",
            "Epoch [84/100], Loss: 0.2988\n",
            "Epoch [85/100], Loss: 0.3086\n",
            "Epoch [86/100], Loss: 0.2911\n",
            "Epoch [87/100], Loss: 0.3039\n",
            "Epoch [88/100], Loss: 0.3566\n",
            "Epoch [89/100], Loss: 0.3302\n",
            "Epoch [90/100], Loss: 0.3333\n",
            "Epoch [91/100], Loss: 0.3661\n",
            "Epoch [92/100], Loss: 0.3289\n",
            "Epoch [93/100], Loss: 0.4122\n",
            "Epoch [94/100], Loss: 0.3872\n",
            "Epoch [95/100], Loss: 0.3836\n",
            "Epoch [96/100], Loss: 0.3622\n",
            "Epoch [97/100], Loss: 0.3984\n",
            "Epoch [98/100], Loss: 0.3705\n",
            "Epoch [99/100], Loss: 0.4755\n",
            "Epoch [100/100], Loss: 0.3832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's process the test set similarly\n",
        "test_file_path = '/content/ddos_data_test.csv'  # Replace with your test file path\n",
        "test_data = pd.read_csv(test_file_path)\n",
        "\n",
        "# Normalizing the 'c4' column in test data (using the same scaler as the training set)\n",
        "test_data['c4'] = scaler.transform(test_data[['c4']])\n",
        "\n",
        "# Creating sequences of length 60 for test data\n",
        "X_test, y_test = create_sequences(test_data, seq_len)\n",
        "\n",
        "# Converting to PyTorch tensors and reshaping for test data\n",
        "X_test_tensor = torch.tensor(X_test).float().unsqueeze(1)\n",
        "y_test_tensor = torch.tensor(y_test).float()\n",
        "\n",
        "# Creating a DataLoader for the test set\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=bs, shuffle=False, drop_last=True)\n",
        "\n",
        "\n",
        "def evaluate_class_wise(model, dataloader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    class_correct = {0: 0, 1: 0}\n",
        "    class_total = {0: 0, 1: 0}\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for xb, yb in dataloader:\n",
        "            outputs = model(xb)\n",
        "            outputs = outputs.squeeze()\n",
        "            predictions = torch.sigmoid(outputs) >= 0.5  # Convert to binary predictions\n",
        "            correct = (predictions == yb)\n",
        "\n",
        "            for label, correct in zip(yb, correct):\n",
        "                label = label.item()\n",
        "                class_correct[label] += correct.item()\n",
        "                class_total[label] += 1\n",
        "\n",
        "    for class_id in class_correct:\n",
        "        if class_total[class_id] > 0:\n",
        "            accuracy = class_correct[class_id] / class_total[class_id]\n",
        "            print(f'Class {class_id} Accuracy: {accuracy:.4f}')\n",
        "        else:\n",
        "            print(f'Class {class_id} has no samples')\n",
        "\n",
        "    return class_correct, class_total\n",
        "\n",
        "\n",
        "# Assuming you have a validation or test DataLoader named 'val_dataloader'\n",
        "evaluate_class_wise(model, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lgt8SwAsJIP",
        "outputId": "d1c257e6-c1e9-497b-9001-e350154fbbf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0 Accuracy: 0.5577\n",
            "Class 1 Accuracy: 0.0185\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({0: 522, 1: 20}, {0: 936, 1: 1080})"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(model, dataloader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for xb, yb in dataloader:\n",
        "            outputs = model(xb)\n",
        "            predicted = torch.sigmoid(outputs.squeeze()) >= 0.5  # Convert to binary predictions\n",
        "            total_correct += (predicted == yb).sum().item()\n",
        "            total_samples += yb.size(0)\n",
        "\n",
        "    accuracy = total_correct / total_samples\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "    return accuracy\n",
        "\n",
        "calculate_accuracy(model, test_dataloader)\n"
      ],
      "metadata": {
        "id": "JeiPn9nfxzIg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32c0f2b1-0f79-4c4e-ece6-08fd53d2708e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.2688\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.26884920634920634"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mlVcqQ2F1wpe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}